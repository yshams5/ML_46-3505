{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64be4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold,train_test_split,StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0af4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.14854131 0.08519616 0.07329723 0.06725241 0.06230734 0.05817919\n",
      " 0.04671073 0.04618429 0.04258886 0.03891183 0.0385629  0.03452099\n",
      " 0.03200254 0.03159968 0.03097948 0.0282619  0.02581058 0.02408586]\n",
      "Total Variance Explained: 91.5\n",
      "             0         1         2         3         4         5         6   \\\n",
      "83925 -1.369625 -0.028261  0.700786 -0.623344 -0.121406 -0.481243  3.551702   \n",
      "13753  0.594599 -1.392285 -0.016241 -0.384470  0.644138 -1.102699  0.341632   \n",
      "29396 -0.214001  0.398832 -1.119655 -0.168287 -1.164556  0.654420 -1.143870   \n",
      "49135  0.406620 -1.240700 -1.593010  0.185908 -0.319906 -1.817363  0.139627   \n",
      "92571 -1.636416 -0.853934  2.671088 -0.563851  0.187265  0.695627 -0.356819   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "83925 -2.219752 -0.142867  1.756871 -2.065923 -2.268459 -1.334403 -3.961910   \n",
      "13753  0.508104 -1.644852 -0.321287  1.005038  0.542841  0.141984 -0.209362   \n",
      "29396 -0.978898 -0.165086  1.992412  0.844742  0.522911 -1.404204  0.618749   \n",
      "49135 -0.555704  0.239909 -0.209519 -0.026011 -0.436123  0.321059  0.067425   \n",
      "92571  0.673596  0.361984 -0.334628 -0.974343 -0.659833 -0.792464  0.150517   \n",
      "\n",
      "             14        15        16        17  \n",
      "83925  0.706768  0.469696 -0.022138  1.407483  \n",
      "13753  0.226937 -0.110043  0.444914  0.442471  \n",
      "29396 -0.555854 -0.295832 -1.260473  0.416686  \n",
      "49135  0.533987 -0.333670 -0.288641  0.170329  \n",
      "92571 -0.085717 -1.110052 -0.462754 -0.709240  \n",
      "83925    1.0\n",
      "13753    2.0\n",
      "29396    2.0\n",
      "49135    2.0\n",
      "92571    2.0\n",
      "Name: accident_severity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Read & Adjust Data from CSV file\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/yshams5/Machine-Learning-Classification-Project/Preprocessing/2017_Accidents_UK_Clean _Mahmoud.csv\",dtype=float)\n",
    "x = data.iloc[:, :-1] \n",
    "y = data.iloc[:, -1]  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "priciple_component_analayzer = PCA(n_components=0.9)  \n",
    "x_pca = priciple_component_analayzer.fit_transform(x_scaled)\n",
    "x=pd.DataFrame(x_pca)\n",
    "print('Explained variance ratio:', priciple_component_analayzer.explained_variance_ratio_)\n",
    "print('Total Variance Explained:', round(sum(list(priciple_component_analayzer.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "\n",
    "\n",
    "class_labels = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y)\n",
    "class_weight_dictionary = {class_labels[i]: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size=0.15,stratify=y)\n",
    "\n",
    "x=x_training\n",
    "y=y_training\n",
    "\n",
    "print(x.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9239071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52817187\n",
      "Iteration 2, loss = 0.51221430\n",
      "Iteration 3, loss = 0.50996942\n",
      "Iteration 4, loss = 0.50884378\n",
      "Iteration 5, loss = 0.50864941\n",
      "Iteration 6, loss = 0.50833155\n",
      "Iteration 7, loss = 0.50724570\n",
      "Iteration 8, loss = 0.50618931\n",
      "Iteration 9, loss = 0.50618932\n",
      "Iteration 10, loss = 0.50623151\n",
      "Iteration 11, loss = 0.50767601\n",
      "Iteration 12, loss = 0.50598867\n",
      "Iteration 13, loss = 0.50463904\n",
      "Iteration 14, loss = 0.50524676\n",
      "Iteration 15, loss = 0.50498593\n",
      "Iteration 16, loss = 0.50478752\n",
      "Iteration 17, loss = 0.50389857\n",
      "Iteration 18, loss = 0.50398653\n",
      "Iteration 19, loss = 0.50395881\n",
      "Iteration 20, loss = 0.50338927\n",
      "Iteration 21, loss = 0.50312762\n",
      "Iteration 22, loss = 0.50335725\n",
      "Iteration 23, loss = 0.50299064\n",
      "Iteration 24, loss = 0.50302518\n",
      "Iteration 25, loss = 0.50285692\n",
      "Iteration 26, loss = 0.50202470\n",
      "Iteration 27, loss = 0.50223157\n",
      "Iteration 28, loss = 0.50200410\n",
      "Iteration 29, loss = 0.50221352\n",
      "Iteration 30, loss = 0.50235969\n",
      "Iteration 31, loss = 0.50195383\n",
      "Iteration 32, loss = 0.50133700\n",
      "Iteration 33, loss = 0.50155319\n",
      "Iteration 34, loss = 0.50139013\n",
      "Iteration 35, loss = 0.50112719\n",
      "Iteration 36, loss = 0.50058204\n",
      "Iteration 37, loss = 0.50088526\n",
      "Iteration 38, loss = 0.50063479\n",
      "Iteration 39, loss = 0.50038581\n",
      "Iteration 40, loss = 0.50064807\n",
      "Iteration 41, loss = 0.50029523\n",
      "Iteration 42, loss = 0.50018337\n",
      "Iteration 43, loss = 0.50032398\n",
      "Iteration 44, loss = 0.49966202\n",
      "Iteration 45, loss = 0.49991795\n",
      "Iteration 46, loss = 0.49922899\n",
      "Iteration 47, loss = 0.49929576\n",
      "Iteration 48, loss = 0.49885036\n",
      "Iteration 49, loss = 0.49866267\n",
      "Iteration 50, loss = 0.49982738\n",
      "Iteration 51, loss = 0.49895119\n",
      "Iteration 52, loss = 0.49888314\n",
      "Iteration 53, loss = 0.49869155\n",
      "Iteration 54, loss = 0.49872566\n",
      "Iteration 55, loss = 0.49916372\n",
      "Iteration 56, loss = 0.49804863\n",
      "Iteration 57, loss = 0.49787060\n",
      "Iteration 58, loss = 0.49850149\n",
      "Iteration 59, loss = 0.49752160\n",
      "Iteration 60, loss = 0.49776142\n",
      "Iteration 61, loss = 0.49896485\n",
      "Iteration 62, loss = 0.49853339\n",
      "Iteration 63, loss = 0.49800164\n",
      "Iteration 64, loss = 0.49787175\n",
      "Iteration 65, loss = 0.49910227\n",
      "Iteration 66, loss = 0.49776117\n",
      "Iteration 67, loss = 0.49826429\n",
      "Iteration 68, loss = 0.49767190\n",
      "Iteration 69, loss = 0.49790034\n",
      "Iteration 70, loss = 0.49909718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.200000\n",
      "Iteration 71, loss = 0.49380928\n",
      "Iteration 72, loss = 0.49088385\n",
      "Iteration 73, loss = 0.48949712\n",
      "Iteration 74, loss = 0.48912864\n",
      "Iteration 75, loss = 0.48852949\n",
      "Iteration 76, loss = 0.48793040\n",
      "Iteration 77, loss = 0.48789716\n",
      "Iteration 78, loss = 0.48745339\n",
      "Iteration 79, loss = 0.48696892\n",
      "Iteration 80, loss = 0.48690429\n",
      "Iteration 81, loss = 0.48660648\n",
      "Iteration 82, loss = 0.48608524\n",
      "Iteration 83, loss = 0.48625793\n",
      "Iteration 84, loss = 0.48595533\n",
      "Iteration 85, loss = 0.48614900\n",
      "Iteration 86, loss = 0.48580802\n",
      "Iteration 87, loss = 0.48574045\n",
      "Iteration 88, loss = 0.48532562\n",
      "Iteration 89, loss = 0.48515058\n",
      "Iteration 90, loss = 0.48531038\n",
      "Iteration 91, loss = 0.48496133\n",
      "Iteration 92, loss = 0.48477912\n",
      "Iteration 93, loss = 0.48511218\n",
      "Iteration 94, loss = 0.48453797\n",
      "Iteration 95, loss = 0.48463808\n",
      "Iteration 96, loss = 0.48468005\n",
      "Iteration 97, loss = 0.48462153\n",
      "Iteration 98, loss = 0.48432474\n",
      "Iteration 99, loss = 0.48431356\n",
      "Iteration 100, loss = 0.48456919\n",
      "Iteration 101, loss = 0.48433338\n",
      "Iteration 102, loss = 0.48414304\n",
      "Iteration 103, loss = 0.48406666\n",
      "Iteration 104, loss = 0.48425270\n",
      "Iteration 105, loss = 0.48359270\n",
      "Iteration 106, loss = 0.48381331\n",
      "Iteration 107, loss = 0.48376317\n",
      "Iteration 108, loss = 0.48377816\n",
      "Iteration 109, loss = 0.48375734\n",
      "Iteration 110, loss = 0.48359449\n",
      "Iteration 111, loss = 0.48341671\n",
      "Iteration 112, loss = 0.48374333\n",
      "Iteration 113, loss = 0.48357153\n",
      "Iteration 114, loss = 0.48318466\n",
      "Iteration 115, loss = 0.48330170\n",
      "Iteration 116, loss = 0.48308355\n",
      "Iteration 117, loss = 0.48301402\n",
      "Iteration 118, loss = 0.48330171\n",
      "Iteration 119, loss = 0.48312601\n",
      "Iteration 120, loss = 0.48281338\n",
      "Iteration 121, loss = 0.48298847\n",
      "Iteration 122, loss = 0.48299007\n",
      "Iteration 123, loss = 0.48281577\n",
      "Iteration 124, loss = 0.48287035\n",
      "Iteration 125, loss = 0.48244434\n",
      "Iteration 126, loss = 0.48241198\n",
      "Iteration 127, loss = 0.48283698\n",
      "Iteration 128, loss = 0.48300631\n",
      "Iteration 129, loss = 0.48283149\n",
      "Iteration 130, loss = 0.48263386\n",
      "Iteration 131, loss = 0.48225566\n",
      "Iteration 132, loss = 0.48280380\n",
      "Iteration 133, loss = 0.48217405\n",
      "Iteration 134, loss = 0.48222822\n",
      "Iteration 135, loss = 0.48210733\n",
      "Iteration 136, loss = 0.48232425\n",
      "Iteration 137, loss = 0.48236140\n",
      "Iteration 138, loss = 0.48239136\n",
      "Iteration 139, loss = 0.48219301\n",
      "Iteration 140, loss = 0.48194724\n",
      "Iteration 141, loss = 0.48212682\n",
      "Iteration 142, loss = 0.48208161\n",
      "Iteration 143, loss = 0.48186317\n",
      "Iteration 144, loss = 0.48171865\n",
      "Iteration 145, loss = 0.48195608\n",
      "Iteration 146, loss = 0.48169077\n",
      "Iteration 147, loss = 0.48188609\n",
      "Iteration 148, loss = 0.48188505\n",
      "Iteration 149, loss = 0.48180298\n",
      "Iteration 150, loss = 0.48180261\n",
      "Iteration 151, loss = 0.48196448\n",
      "Iteration 152, loss = 0.48154276\n",
      "Iteration 153, loss = 0.48211648\n",
      "Iteration 154, loss = 0.48127628\n",
      "Iteration 155, loss = 0.48128287\n",
      "Iteration 156, loss = 0.48128915\n",
      "Iteration 157, loss = 0.48093052\n",
      "Iteration 158, loss = 0.48099435\n",
      "Iteration 159, loss = 0.48067671\n",
      "Iteration 160, loss = 0.48090399\n",
      "Iteration 161, loss = 0.48121983\n",
      "Iteration 162, loss = 0.48162185\n",
      "Iteration 163, loss = 0.48100287\n",
      "Iteration 164, loss = 0.48120586\n",
      "Iteration 165, loss = 0.48109720\n",
      "Iteration 166, loss = 0.48043649\n",
      "Iteration 167, loss = 0.48051898\n",
      "Iteration 168, loss = 0.48105540\n",
      "Iteration 169, loss = 0.48101472\n",
      "Iteration 170, loss = 0.48125977\n",
      "Iteration 171, loss = 0.48064943\n",
      "Iteration 172, loss = 0.48013289\n",
      "Iteration 173, loss = 0.48013641\n",
      "Iteration 174, loss = 0.48075383\n",
      "Iteration 175, loss = 0.48027752\n",
      "Iteration 176, loss = 0.48021964\n",
      "Iteration 177, loss = 0.48068698\n",
      "Iteration 178, loss = 0.48058222\n",
      "Iteration 179, loss = 0.48012794\n",
      "Iteration 180, loss = 0.48014558\n",
      "Iteration 181, loss = 0.48007510\n",
      "Iteration 182, loss = 0.47989937\n",
      "Iteration 183, loss = 0.48015624\n",
      "Iteration 184, loss = 0.48107318\n",
      "Iteration 185, loss = 0.47988863\n",
      "Iteration 186, loss = 0.48053822\n",
      "Iteration 187, loss = 0.47992690\n",
      "Iteration 188, loss = 0.48024875\n",
      "Iteration 189, loss = 0.47992501\n",
      "Iteration 190, loss = 0.47997220\n",
      "Iteration 191, loss = 0.48005248\n",
      "Iteration 192, loss = 0.48030405\n",
      "Iteration 193, loss = 0.47983261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.040000\n",
      "Iteration 194, loss = 0.47726396\n",
      "Iteration 195, loss = 0.47371128\n",
      "Iteration 196, loss = 0.47280472\n",
      "Iteration 197, loss = 0.47219982\n",
      "Iteration 198, loss = 0.47200032\n",
      "Iteration 199, loss = 0.47168746\n",
      "Iteration 200, loss = 0.47129886\n",
      "Iteration 201, loss = 0.47117905\n",
      "Iteration 202, loss = 0.47097549\n",
      "Iteration 203, loss = 0.47094332\n",
      "Iteration 204, loss = 0.47080393\n",
      "Iteration 205, loss = 0.47053419\n",
      "Iteration 206, loss = 0.47059468\n",
      "Iteration 207, loss = 0.47035212\n",
      "Iteration 208, loss = 0.47047298\n",
      "Iteration 209, loss = 0.47016974\n",
      "Iteration 210, loss = 0.47039449\n",
      "Iteration 211, loss = 0.47002644\n",
      "Iteration 212, loss = 0.47006914\n",
      "Iteration 213, loss = 0.46993011\n",
      "Iteration 214, loss = 0.46988886\n",
      "Iteration 215, loss = 0.46997378\n",
      "Iteration 216, loss = 0.46987299\n",
      "Iteration 217, loss = 0.46974173\n",
      "Iteration 218, loss = 0.46962945\n",
      "Iteration 219, loss = 0.46954843\n",
      "Iteration 220, loss = 0.46933829\n",
      "Iteration 221, loss = 0.46938828\n",
      "Iteration 222, loss = 0.46914634\n",
      "Iteration 223, loss = 0.46930584\n",
      "Iteration 224, loss = 0.46926413\n",
      "Iteration 225, loss = 0.46923758\n",
      "Iteration 226, loss = 0.46922036\n",
      "Iteration 227, loss = 0.46920918\n",
      "Iteration 228, loss = 0.46955629\n",
      "Iteration 229, loss = 0.46936284\n",
      "Iteration 230, loss = 0.46926231\n",
      "Iteration 231, loss = 0.46908335\n",
      "Iteration 232, loss = 0.46906853\n",
      "Iteration 233, loss = 0.46883067\n",
      "Iteration 234, loss = 0.46890791\n",
      "Iteration 235, loss = 0.46890713\n",
      "Iteration 236, loss = 0.46896904\n",
      "Iteration 237, loss = 0.46890465\n",
      "Iteration 238, loss = 0.46881287\n",
      "Iteration 239, loss = 0.46903109\n",
      "Iteration 240, loss = 0.46894853\n",
      "Iteration 241, loss = 0.46882160\n",
      "Iteration 242, loss = 0.46866657\n",
      "Iteration 243, loss = 0.46895829\n",
      "Iteration 244, loss = 0.46886245\n",
      "Iteration 245, loss = 0.46859714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 246, loss = 0.46861869\n",
      "Iteration 247, loss = 0.46878638\n",
      "Iteration 248, loss = 0.46862697\n",
      "Iteration 249, loss = 0.46877976\n",
      "Iteration 250, loss = 0.46838379\n",
      "Iteration 251, loss = 0.46908198\n",
      "Iteration 252, loss = 0.46870563\n",
      "Iteration 253, loss = 0.46870173\n",
      "Iteration 254, loss = 0.46834663\n",
      "Iteration 255, loss = 0.46866269\n",
      "Iteration 256, loss = 0.46881284\n",
      "Iteration 257, loss = 0.46868731\n",
      "Iteration 258, loss = 0.46855453\n",
      "Iteration 259, loss = 0.46865290\n",
      "Iteration 260, loss = 0.46852756\n",
      "Iteration 261, loss = 0.46848769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.008000\n",
      "Iteration 262, loss = 0.46668220\n",
      "Iteration 263, loss = 0.46510389\n",
      "Iteration 264, loss = 0.46465719\n",
      "Iteration 265, loss = 0.46440134\n",
      "Iteration 266, loss = 0.46427050\n",
      "Iteration 267, loss = 0.46417774\n",
      "Iteration 268, loss = 0.46408558\n",
      "Iteration 269, loss = 0.46392224\n",
      "Iteration 270, loss = 0.46388632\n",
      "Iteration 271, loss = 0.46383872\n",
      "Iteration 272, loss = 0.46371929\n",
      "Iteration 273, loss = 0.46371446\n",
      "Iteration 274, loss = 0.46364226\n",
      "Iteration 275, loss = 0.46357612\n",
      "Iteration 276, loss = 0.46354802\n",
      "Iteration 277, loss = 0.46349503\n",
      "Iteration 278, loss = 0.46351486\n",
      "Iteration 279, loss = 0.46344803\n",
      "Iteration 280, loss = 0.46337847\n",
      "Iteration 281, loss = 0.46335835\n",
      "Iteration 282, loss = 0.46337478\n",
      "Iteration 283, loss = 0.46335550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.001600\n",
      "Iteration 284, loss = 0.46271343\n",
      "Iteration 285, loss = 0.46248877\n",
      "Iteration 286, loss = 0.46240383\n",
      "Iteration 287, loss = 0.46234501\n",
      "Iteration 288, loss = 0.46234356\n",
      "Iteration 289, loss = 0.46230874\n",
      "Iteration 290, loss = 0.46228913\n",
      "Iteration 291, loss = 0.46225255\n",
      "Iteration 292, loss = 0.46224239\n",
      "Iteration 293, loss = 0.46224204\n",
      "Iteration 294, loss = 0.46220568\n",
      "Iteration 295, loss = 0.46221125\n",
      "Iteration 296, loss = 0.46217473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000320\n",
      "Iteration 297, loss = 0.46204809\n",
      "Iteration 298, loss = 0.46201042\n",
      "Iteration 299, loss = 0.46199703\n",
      "Iteration 300, loss = 0.46199020\n",
      "Iteration 301, loss = 0.46198333\n",
      "Iteration 302, loss = 0.46197813\n",
      "Iteration 303, loss = 0.46197235\n",
      "Iteration 304, loss = 0.46196973\n",
      "Iteration 305, loss = 0.46196521\n",
      "Iteration 306, loss = 0.46196143\n",
      "Iteration 307, loss = 0.46195941\n",
      "Iteration 308, loss = 0.46195440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000064\n",
      "Iteration 309, loss = 0.46192154\n",
      "Iteration 310, loss = 0.46191802\n",
      "Iteration 311, loss = 0.46191639\n",
      "Iteration 312, loss = 0.46191499\n",
      "Iteration 313, loss = 0.46191363\n",
      "Iteration 314, loss = 0.46191259\n",
      "Iteration 315, loss = 0.46191168\n",
      "Iteration 316, loss = 0.46191047\n",
      "Iteration 317, loss = 0.46190997\n",
      "Iteration 318, loss = 0.46190917\n",
      "Iteration 319, loss = 0.46190844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000013\n",
      "Iteration 320, loss = 0.46190061\n",
      "Iteration 321, loss = 0.46190009\n",
      "Iteration 322, loss = 0.46189974\n",
      "Iteration 323, loss = 0.46189955\n",
      "Iteration 324, loss = 0.46189933\n",
      "Iteration 325, loss = 0.46189904\n",
      "Iteration 326, loss = 0.46189898\n",
      "Iteration 327, loss = 0.46189866\n",
      "Iteration 328, loss = 0.46189849\n",
      "Iteration 329, loss = 0.46189832\n",
      "Iteration 330, loss = 0.46189823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 331, loss = 0.46189664\n",
      "Iteration 332, loss = 0.46189655\n",
      "Iteration 333, loss = 0.46189653\n",
      "Iteration 334, loss = 0.46189646\n",
      "Iteration 335, loss = 0.46189643\n",
      "Iteration 336, loss = 0.46189637\n",
      "Iteration 337, loss = 0.46189635\n",
      "Iteration 338, loss = 0.46189630\n",
      "Iteration 339, loss = 0.46189626\n",
      "Iteration 340, loss = 0.46189626\n",
      "Iteration 341, loss = 0.46189618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 342, loss = 0.46189589\n",
      "Iteration 343, loss = 0.46189588\n",
      "Iteration 344, loss = 0.46189587\n",
      "Iteration 345, loss = 0.46189586\n",
      "Iteration 346, loss = 0.46189585\n",
      "Iteration 347, loss = 0.46189584\n",
      "Iteration 348, loss = 0.46189583\n",
      "Iteration 349, loss = 0.46189583\n",
      "Iteration 350, loss = 0.46189583\n",
      "Iteration 351, loss = 0.46189581\n",
      "Iteration 352, loss = 0.46189581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "{'accuracy': 0.8122120746787359, 'precision': 0.8122120746787359, 'recall': 0.8122120746787359}\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(hidden_layer_sizes=(30,50,10,10,10), activation='relu', batch_size=1000,\n",
    "                          solver='sgd', learning_rate_init=1,learning_rate ='adaptive', max_iter=500,verbose=True)\n",
    "mlp_model.fit(x_training, y_training)\n",
    "y_pred = mlp_model.predict(x_testing)\n",
    "\n",
    "accuracy = accuracy_score(y_testing, y_pred)\n",
    "precision = precision_score(y_testing, y_pred, average='micro', zero_division=0)\n",
    "recall = recall_score(y_testing, y_pred, average='micro')\n",
    "\n",
    "testx = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision' : precision,\n",
    "    'recall': recall\n",
    "}\n",
    "print(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b72b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'learning_rate': 1, 'accuracy': 0.8102723672512729, 'precision': 0.8102723672512729, 'recall': 0.8102723672512729}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.1, 'accuracy': 0.81002990382284, 'precision': 0.81002990382284, 'recall': 0.81002990382284}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.01, 'accuracy': 0.8102723672512729, 'precision': 0.8102723672512729, 'recall': 0.8102723672512729}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.001, 'accuracy': 0.8099490826800291, 'precision': 0.8099490826800291, 'recall': 0.8099490826800291}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'learning_rate': 1, 'accuracy': 0.8103935989654893, 'precision': 0.8103935989654893, 'recall': 0.8103935989654893}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.1, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.01, 'accuracy': 0.8107572941081387, 'precision': 0.8107572941081387, 'recall': 0.8107572941081387}\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.001, 'accuracy': 0.8109997575365716, 'precision': 0.8109997575365716, 'recall': 0.8109997575365716}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'relu', 'learning_rate': 1, 'accuracy': 0.8052614563969934, 'precision': 0.8052614563969934, 'recall': 0.8052614563969934}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'relu', 'learning_rate': 0.1, 'accuracy': 0.8063929523963469, 'precision': 0.8063929523963469, 'recall': 0.8063929523963469}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'relu', 'learning_rate': 0.01, 'accuracy': 0.8057059726824537, 'precision': 0.8057059726824537, 'recall': 0.8057059726824537}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'relu', 'learning_rate': 0.001, 'accuracy': 0.8058272043966702, 'precision': 0.8058272043966702, 'recall': 0.8058272043966702}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'tanh', 'learning_rate': 1, 'accuracy': 0.799159460114766, 'precision': 0.799159460114766, 'recall': 0.799159460114766}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'tanh', 'learning_rate': 0.1, 'accuracy': 0.8040895498262346, 'precision': 0.8040895498262346, 'recall': 0.8040895498262346}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'tanh', 'learning_rate': 0.01, 'accuracy': 0.7978663218297907, 'precision': 0.7978663218297907, 'recall': 0.7978663218297907}\n",
      "{'hidden_layer_sizes': (50, 50), 'activation': 'tanh', 'learning_rate': 0.001, 'accuracy': 0.7987149438293057, 'precision': 0.7987149438293057, 'recall': 0.7987149438293057}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'relu', 'learning_rate': 1, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'relu', 'learning_rate': 0.1, 'accuracy': 0.8106360623939223, 'precision': 0.8106360623939223, 'recall': 0.8106360623939223}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'relu', 'learning_rate': 0.01, 'accuracy': 0.8101107249656511, 'precision': 0.8101107249656511, 'recall': 0.8101107249656511}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'relu', 'learning_rate': 0.001, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'tanh', 'learning_rate': 1, 'accuracy': 0.8086155338236483, 'precision': 0.8086155338236483, 'recall': 0.8086155338236483}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'tanh', 'learning_rate': 0.1, 'accuracy': 0.8086559443950537, 'precision': 0.8086559443950537, 'recall': 0.8086559443950537}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'tanh', 'learning_rate': 0.01, 'accuracy': 0.8095853875373797, 'precision': 0.8095853875373797, 'recall': 0.8095853875373797}\n",
      "{'hidden_layer_sizes': (25, 25, 25), 'activation': 'tanh', 'learning_rate': 0.001, 'accuracy': 0.8086963549664592, 'precision': 0.8086963549664592, 'recall': 0.8086963549664592}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'relu', 'learning_rate': 1, 'accuracy': 0.8053826881112099, 'precision': 0.8053826881112099, 'recall': 0.8053826881112099}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'relu', 'learning_rate': 0.1, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'relu', 'learning_rate': 0.01, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'relu', 'learning_rate': 0.001, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'tanh', 'learning_rate': 1, 'accuracy': 0.8059484361108866, 'precision': 0.8059484361108866, 'recall': 0.8059484361108866}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'tanh', 'learning_rate': 0.1, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'tanh', 'learning_rate': 0.01, 'accuracy': 0.8107168835367332, 'precision': 0.8107168835367332, 'recall': 0.8107168835367332}\n",
      "{'hidden_layer_sizes': (30, 50, 10, 10, 10), 'activation': 'tanh', 'learning_rate': 0.001, 'accuracy': 0.8103935989654893, 'precision': 0.8103935989654893, 'recall': 0.8103935989654893}\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_options = [(100,), (50, 50), (25, 25, 25),(30,50,10,10,10)]\n",
    "activation_options = ['relu', 'tanh']\n",
    "learning_rate_values = [1,1e-1,1e-2,1e-3]\n",
    "results = []\n",
    "\n",
    "for hidden_layer_sizes in hidden_layer_sizes_options:\n",
    "    for activation in activation_options:\n",
    "        for learning_rate in learning_rate_values:\n",
    "            mlp_model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, batch_size=1000,\n",
    "                          solver='sgd', learning_rate_init=1,learning_rate ='adaptive', max_iter=500)\n",
    "            mlp_model.fit(x_training, y_training)\n",
    "            y_pred = mlp_model.predict(x_testing)\n",
    "\n",
    "            accuracy = accuracy_score(y_testing, y_pred)\n",
    "            precision = precision_score(y_testing, y_pred, average='micro', zero_division=0)\n",
    "            recall = recall_score(y_testing, y_pred, average='micro')\n",
    "\n",
    "            this_result = {\n",
    "                'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                'activation': activation,\n",
    "                'learning_rate': learning_rate,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "            results.append(this_result)\n",
    "            print(this_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064a470",
   "metadata": {},
   "source": [
    "# Occam's razor: the simplest solution is the best\n",
    "### Since all the architectures and different hyperparameters produced similar results, taking the simplest solution is appropriate\n",
    "### Trying an even simpler solution to see if it will have the same result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ce06ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8119612068965517\n",
      "Precision: 0.8119612068965517\n",
      "Recall: 0.8119612068965517\n",
      "Confusion Matrix:\n",
      "True Fatal:  0 False Serious:  0 False Slight:  246\n",
      "False Fatal:  0 True Serious:  0 False Slight:  3244\n",
      "False Fatal:  0 False Serious:  0 True Slight:  15070\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(hidden_layer_sizes=(10,10), activation='relu', batch_size=1000,\n",
    "                          solver='sgd', learning_rate_init=1e-3, max_iter=500)\n",
    "mlp_model.fit(x_training, y_training)\n",
    "y_prediction = mlp_model.predict(x_testing)\n",
    "\n",
    "accuracy = accuracy_score(y_testing, y_prediction)\n",
    "precision = precision_score(y_testing, y_prediction, average='micro', zero_division=0)\n",
    "recall = recall_score(y_testing, y_prediction, average='micro')\n",
    "\n",
    "my_confusion_matrix = confusion_matrix(y_testing, y_prediction, labels=[0, 1, 2])\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"True Fatal: \" , my_confusion_matrix[0,0],  \"False Serious: \", my_confusion_matrix[0,1], \"False Slight: \",my_confusion_matrix[0,2] )\n",
    "print(\"False Fatal: \" , my_confusion_matrix[1,0],  \"True Serious: \", my_confusion_matrix[1,1], \"False Slight: \",my_confusion_matrix[1,2])\n",
    "print(\"False Fatal: \" , my_confusion_matrix[2,0],  \"False Serious: \", my_confusion_matrix[2,1], \"True Slight: \",my_confusion_matrix[2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc072d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
